{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276b6dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' --------------------------------------------------------------------'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# All Models \n",
    "\"\"\"\n",
    "combining speech recognition, response generation, emotion recognition models\n",
    "talk --> speech to text --> create response --> detect emotion --> save to files\n",
    "\n",
    "UDP connection is not used here\n",
    "we save outputs in files and global variables\n",
    "outputs: emotion and response sound file\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import time, logging\n",
    "from datetime import datetime\n",
    "import collections, queue, os, os.path\n",
    "\n",
    "import itertools\n",
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "import random\n",
    "import sched\n",
    "\n",
    "import argparse\n",
    "import shutil, os\n",
    "\n",
    "## imports for parlai blender chatbot\n",
    "\n",
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.worlds import create_task\n",
    "from parlai.core.script import ParlaiScript, register_script\n",
    "from parlai.utils.world_logging import WorldLogger\n",
    "import parlai.utils.logging as parlaiLogging\n",
    "\n",
    "from typing import Optional\n",
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.opt import Opt\n",
    "from parlai.core.agents import Agent\n",
    "from parlai.core.message import Message\n",
    "from parlai.utils.misc import display_messages, load_cands\n",
    "from parlai.utils.strings import colorize\n",
    "\n",
    "# imports for Speech Recognition using the Speech SDK\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# imports for synthesizing text using the Speech SDK\n",
    "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
    "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc227f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' --------------------------------------------------------------------'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### load models #####\n",
    "\n",
    "main_path = 'C:/Users/Munia/Documents/Anaconda/Master Thesis Project/vh-models/'\n",
    "\n",
    "\n",
    "# load text based emotion recognition model\n",
    "t_predictor_path = main_path + \"emotion-recognition/emotion recognition from text/Bert/models/bert_model/\"\n",
    "t_predictor = ktrain.load_predictor(t_predictor_path)\n",
    "\n",
    "# load video based emotion recognition models\n",
    "v_detection_model_path = main_path + 'emotion-recognition/real time emotion detection/haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "v_emotion_model_path = main_path + 'emotion-recognition/real time emotion detection/models/XCEPTION-102-0.66.hdf5'\n",
    "face_detection = cv2.CascadeClassifier(v_detection_model_path)\n",
    "v_emotion_classifier = load_model(v_emotion_model_path, compile=False)\n",
    "\n",
    "# load chatbot model for response generation\n",
    "chatbot_model = 'open-domain-chatbot/models/blender/blender_90M/model'\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ff6057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' --------------------------------------------------------------------'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### speech to text #####\n",
    "\n",
    "###### speech reconition using Microsoft Speech API #####\n",
    "\n",
    "## Speech Recognition Using the Speech SDK\n",
    "\n",
    "def speak():\n",
    "    global sentence\n",
    "    speech_key, service_region = \"5b49444222a544a39f55e5544466cd82\", \"eastasia\" # must change API key and region name\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "    print(\"say something: \")\n",
    "    result = speech_recognizer.recognize_once()\n",
    "    if result.reason == speechsdk.ResultReason. RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(result.text))\n",
    "        sentence = result.text\n",
    "        output_file = open(main_path + \"outputs/text-output.txt\", \"w\")\n",
    "        output_file.write(result.text)\n",
    "        output_file.close()\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "        print(\"assigning sentence as -> say something\")\n",
    "        sentence = \"say something\"\n",
    "        output_file = open(main_path + \"outputs/text-output.txt\", \"w\")\n",
    "        output_file.write(sentence)\n",
    "        output_file.close()\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    \n",
    "    \n",
    "# loading animation\n",
    "def spinner():\n",
    "    \n",
    "    done = False\n",
    "    #here is the animation\n",
    "    def animate():\n",
    "        for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
    "            if done:\n",
    "                break\n",
    "            sys.stdout.write('\\rloading ' + c)\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.1)\n",
    "        sys.stdout.write('\\rDone!     ')\n",
    "\n",
    "    t = threading.Thread(target=animate)\n",
    "    t.start()\n",
    "\n",
    "    #long process here\n",
    "    time.sleep(10)\n",
    "    done = True\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n",
    "###### text to speech with Microsoft Speech API #####\n",
    "\n",
    "def synthesize(responsetxt):\n",
    "    \n",
    "    #file=\"C:/Users/Munia/Documents/Unity/VHProject-master-Copy/Assets/Resources/speech.wav\"\n",
    "    file = \"outputs/speech.wav\"\n",
    "    speech_config = SpeechConfig(subscription=\"5b49444222a544a39f55e5544466cd82\", region=\"eastasia\")\n",
    "    #audio_config = AudioOutputConfig(filename=\"outputs/speech.wav\")\n",
    "    audio_config = AudioOutputConfig(filename=file)\n",
    "    \n",
    "    synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "    synthesizer.speak_text_async(responsetxt)\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n",
    "##### emotion recognition #####\n",
    "\n",
    "### from text\n",
    "\n",
    "def text_based_emotion(text):\n",
    "    global t_emotion\n",
    "    global t_prob\n",
    "    EMOTIONS = ['happy', 'sad', 'scared', 'angry', 'neutral']\n",
    "    \n",
    "    start_time = time.time() \n",
    "    preds = t_predictor.predict(text, return_proba=True)\n",
    "    emotion_probability = np.max(preds)\n",
    "    emotion_probability = str(round(emotion_probability, 2))\n",
    "    #convert probability from string to float\n",
    "    probability = float(emotion_probability)\n",
    "    #print(\"blah blah blah: \", probability)\n",
    "    label = EMOTIONS[preds.argmax()]\n",
    "\n",
    "    #print('predicted: {} {} ({:.2f})'.format(label, emotion_probability, (time.time() - start_time)))\n",
    "    t_emotion = label\n",
    "    t_prob = probability\n",
    "    \n",
    "    #return label, emotion_probability\n",
    "\n",
    "\n",
    "### from video\n",
    "\n",
    "# video capture is set to play for 5 seconds\n",
    "def video_based_emotion():\n",
    "    \n",
    "    #main_path = 'C:/Users/Munia/Documents/Anaconda/vh-models/emotion-recognition/real time emotion detection/'\n",
    "    global v_emotion\n",
    "    global v_prob\n",
    "    \n",
    "    EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\", \"neutral\"]\n",
    "\n",
    "    cv2.namedWindow('your_face')\n",
    "    t0 = time.time() # start time in seconds\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        frame = camera.read()[1]\n",
    "        #reading the frame\n",
    "        frame = imutils.resize(frame,width=500)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "        frameClone = frame.copy()\n",
    "        if len(faces) > 0:\n",
    "            faces = sorted(faces, reverse=True,\n",
    "            key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "            (fX, fY, fW, fH) = faces\n",
    "\n",
    "            roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "            roi = cv2.resize(roi, (64, 64))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "\n",
    "            preds = v_emotion_classifier.predict(roi)[0]\n",
    "            #print(preds)\n",
    "            emotion_probability = np.max(preds)\n",
    "            #print(np.max(preds))\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "            #print(label)\n",
    "        else: continue\n",
    "\n",
    "        for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                    # construct the label text\n",
    "                    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                    w = int(prob * 300)\n",
    "                    cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
    "                    (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                    cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                    (255, 255, 255), 2)\n",
    "                    cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "                    cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                                  (0, 0, 255), 2)\n",
    "        t1 = time.time() # current time\n",
    "        num_seconds = t1 - t0 # diff\n",
    "\n",
    "        cv2.imshow('your_face', frameClone)\n",
    "        cv2.imshow(\"Probabilities\", canvas)\n",
    "        \n",
    "        # stop video capture after x seconds\n",
    "        if num_seconds > 5:  # e.g. break after 30 seconds\n",
    "            break\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    v_emotion = label\n",
    "    v_prob = emotion_probability\n",
    "    \n",
    "### predict final emotion\n",
    "\n",
    "# last function for emotion detection that comapres between emotion from video and emotion from text\n",
    "def predict_emotion():\n",
    "    emotionsList = [\"happy\", \"sad\", \"neutral\", \"angry\", \"scared\", \"surprised\", \"disgust\"]\n",
    "    final_emotion = \"\"\n",
    "    final_prob = 0\n",
    "    \n",
    "    if (t_emotion==v_emotion):\n",
    "        final_emotion = t_emotion\n",
    "        final_prob = t_prob\n",
    "        return final_emotion, final_prob\n",
    "    else:\n",
    "        if (v_prob is not None):\n",
    "            MAX = max(t_prob, v_prob) \n",
    "            if(MAX == t_prob):\n",
    "                final_emotion = t_emotion\n",
    "                final_prob = t_prob\n",
    "                return final_emotion, final_prob\n",
    "            elif(MAX == v_prob):\n",
    "                final_emotion = v_emotion\n",
    "                final_prob = v_prob\n",
    "                return final_emotion, final_prob\n",
    "        else:\n",
    "            final_emotion = t_emotion\n",
    "            final_prob = t_prob\n",
    "            return final_emotion, final_prob\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n",
    "##### generate response #####\n",
    "### response generation with ParlAI Blender 90M Model\n",
    "\n",
    "\"\"\"\n",
    "LocalHumanAgent Class (for keyboard input) to let human reply replacing an ML agent\n",
    "instead of importing  LocalHumanAgent.py Script\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Agent does gets the local keyboard input in the act() function.\n",
    "Example: parlai eval_model -m local_human -t babi:Task1k:1 -dt valid\n",
    "\"\"\"\n",
    "class LocalHumanAgent(Agent):\n",
    "    @classmethod\n",
    "    def add_cmdline_args(\n",
    "        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None\n",
    "    ) -> ParlaiParser:\n",
    "        \"\"\"\n",
    "        Add command-line arguments specifically for this agent.\n",
    "        \"\"\"\n",
    "        agent = parser.add_argument_group('Local Human Arguments')\n",
    "        agent.add_argument(\n",
    "            '-fixedCands',\n",
    "            '--local-human-candidates-file',\n",
    "            default=None,\n",
    "            type=str,\n",
    "            help='File of label_candidates to send to other agent',\n",
    "        )\n",
    "        agent.add_argument(\n",
    "            '--single_turn',\n",
    "            type='bool',\n",
    "            default=False,\n",
    "            help='If on, assumes single turn episodes.',\n",
    "        )\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt, shared=None):\n",
    "        super().__init__(opt)\n",
    "        self.id = 'localHuman'\n",
    "        self.episodeDone = False\n",
    "        self.finished = False\n",
    "        self.fixedCands_txt = load_cands(self.opt.get('local_human_candidates_file'))\n",
    "        print(\n",
    "            colorize(\n",
    "                \"Enter [DONE] if you want to end the episode, [EXIT] to quit.\",\n",
    "                'highlight',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def epoch_done(self):\n",
    "        return self.finished\n",
    "\n",
    "    def observe(self, msg):\n",
    "        print(\n",
    "            display_messages(\n",
    "                [msg],\n",
    "                add_fields=self.opt.get('display_add_fields', ''),\n",
    "                prettify=self.opt.get('display_prettify', False),\n",
    "                verbose=self.opt.get('verbose', False),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def act(self):\n",
    "        reply = Message()\n",
    "        reply['id'] = self.getID()\n",
    "        try:\n",
    "            ## input message from user\n",
    "            #reply_text = input(colorize(\"Enter Your Message:\", 'text') + ' ')\n",
    "            reply_text = sentence\n",
    "        except EOFError:\n",
    "            self.finished = True\n",
    "            return {'episode_done': True}\n",
    "\n",
    "        #reply_text = reply_text.replace('\\\\n', '\\n')\n",
    "        reply['episode_done'] = False\n",
    "        if self.opt.get('single_turn', False):\n",
    "            reply.force_set('episode_done', True)\n",
    "        reply['label_candidates'] = self.fixedCands_txt\n",
    "        reply['text'] = reply_text\n",
    "        print(\"reply['text'] is = \", reply['text'])\n",
    "        print(\"reply is = \", reply)\n",
    "      \n",
    "        return reply\n",
    "\n",
    "    def episode_done(self):\n",
    "        return self.episodeDone\n",
    "\n",
    "\"\"\"\n",
    "Basic script which allows local human keyboard input to talk to a trained model.\n",
    "\n",
    "Model = Blender 90M\n",
    "\"\"\"\n",
    "\n",
    "def setup_args(parser=None):\n",
    "    if parser is None:\n",
    "        parser = ParlaiParser(\n",
    "            True, True, 'Interactive chat with a model on the command line'\n",
    "        )\n",
    "    parser.add_argument('-d', '--display-examples', type='bool', default=False)\n",
    "    parser.add_argument(\n",
    "        '--display-prettify',\n",
    "        type='bool',\n",
    "        default=False,\n",
    "        help='Set to use a prettytable when displaying '\n",
    "        'examples with text candidates',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--display-add-fields',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Display these fields when verbose is off (e.g., \"--display-add-fields label_candidates,beam_texts\")',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-it',\n",
    "        '--interactive-task',\n",
    "        type='bool',\n",
    "        default=True,\n",
    "        help='Create interactive version of task',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--outfile',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Saves a jsonl file containing all of the task examples and '\n",
    "        'model replies. Set to the empty string to not save at all',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save-format',\n",
    "        type=str,\n",
    "        default='conversations',\n",
    "        choices=['conversations', 'parlai'],\n",
    "        help='Format to save logs in. conversations is a jsonl format, parlai is a text format.',\n",
    "    )\n",
    "    parser.set_defaults(interactive_mode=True, task='interactive')\n",
    "    LocalHumanAgent.add_cmdline_args(parser, partial_opt=None)\n",
    "    WorldLogger.add_cmdline_args(parser, partial_opt=None)\n",
    "    return parser\n",
    "\n",
    "##### main function for chtabot to build a chat(from interactive script) \n",
    "\n",
    "def interactive(opt):\n",
    "    global response\n",
    "    if isinstance(opt, ParlaiParser):\n",
    "        parlaiLogging.error('interactive should be passed opt not Parser')\n",
    "        opt = opt.parse_args()\n",
    "\n",
    "    # Create model and assign it to the specified task\n",
    "    agent = create_agent(opt, requireModelExists=True)\n",
    "    #agent.opt.log()\n",
    "    human_agent = LocalHumanAgent(opt)\n",
    "    # set up world logger\n",
    "    world_logger = WorldLogger(opt) if opt.get('outfile') else None\n",
    "    world = create_task(opt, [human_agent, agent])\n",
    "\n",
    "    # Show some example dialogs:\n",
    "    world.parley()\n",
    "    #world.get_acts()\n",
    "    response = world.get_acts()[1]['text']\n",
    "    #print(world.get_acts()[1]['text'])\n",
    "    #a = world.get_acts()\n",
    "\n",
    "@register_script('interactive', aliases=['i'])\n",
    "class Interactive(ParlaiScript):\n",
    "    @classmethod\n",
    "    def setup_args(cls):\n",
    "        return setup_args()\n",
    "\n",
    "    def run(self):\n",
    "        return interactive(self.opt)\n",
    "\n",
    "##### Generate Response Function, calles the interactive function\n",
    "def responseGenerate():\n",
    "    random.seed(42)\n",
    "    Interactive.main(model_file=chatbot_model)\n",
    "\n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e769c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6a945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say something: \n",
      "loading -Recognized: Hello, how are you?\n",
      "Done!     13:03:54 | \u001b[33mOverriding opt[\"model_file\"] to open-domain-chatbot/models/blender/blender_90M/model (previously: /checkpoint/edinan/20200210/baseline_BST_retnref/lr=7.5e-06_attention-dropout=0.0_relu-dropout=0.0/model)\u001b[0m\n",
      "13:03:54 | loading dictionary from open-domain-chatbot/models/blender/blender_90M/model.dict\n",
      "13:03:55 | num words = 54944\n",
      "13:03:55 | TransformerGenerator: full interactive mode on.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### Main Code // run all models #####\n",
    "\n",
    "##### function to print the resulted chat showing what user said, response, emotions\n",
    "\n",
    "def  print_dialog():\n",
    "    print(\"\\n\\n____________ Dialog ____________\")\n",
    "    print(\"Me: \", sentence)\n",
    "    print(\"emotion from video is: \", v_emotion)\n",
    "    print(\"probability  is: \", v_prob)\n",
    "\n",
    "    print(\"Bot: \", response)\n",
    "    print(\"emotion from text is: \", t_emotion)\n",
    "    print(\"probability is: \", t_prob)\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Outputs: \")\n",
    "    print(\"final emotion output is: \", final_emo, \"with probability: \", final_prob)\n",
    "    print(\"bot response output is: \", response)\n",
    "\n",
    "    \n",
    "\"\"\" --------------------------------------------------------------------\"\"\"\n",
    "\n",
    "final_emo = None\n",
    "t_emotion = None\n",
    "t_prob = None\n",
    "v_emotion = None\n",
    "v_prob = None\n",
    "sentence = None\n",
    "response = None\n",
    "\n",
    "\n",
    "# call speech recognition, with animation, and video based emotion functions together in parallel with threads\n",
    "# store spoken sentence and video emotion in gloabal variables\n",
    "# call generate response and store the response in gloabal variables\n",
    "# call text based emotion on response\n",
    "# call synthesize function to convert response text to speech and store it as file\n",
    "# print the dialog\n",
    "# call the predict emotion funcction to pick final emotion \n",
    "    #compare between video and text emotions and pick the one with lareger probability\n",
    "    # store the emotion as file\n",
    "# wait for seconds then repeat again\n",
    "\n",
    "while True:\n",
    "    \n",
    "    t1 = Thread(target = speak)\n",
    "    t2 = Thread(target = spinner)\n",
    "    t3 = Thread(target = video_based_emotion)\n",
    "    \n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    \n",
    "    responseGenerate()\n",
    "    text_based_emotion(response)\n",
    "        \n",
    "    final_emo, final_prob = predict_emotion()\n",
    "    synthesize(response)\n",
    "\n",
    "    print_dialog()\n",
    "\n",
    "    # saving emotion in outputs folder // speech has been saved in the synthesize function\n",
    "    output_emotion = open(main_path + \"outputs/emotion.txt\", \"w\")\n",
    "    output_emotion.write(final_emo)\n",
    "    output_emotion.close()\n",
    "\n",
    "    # saving emotion and speech in Unity's Resources folder\n",
    "\n",
    "    directory = \"C:/Users/Munia/Documents/Unity/VHProject-master-Copy/Assets/Resources/\"\n",
    "\n",
    "    output_emotion = open(directory + \"emotion.txt\", \"w\")\n",
    "    output_emotion.write(final_emo)\n",
    "    output_emotion.close()\n",
    "    \n",
    "    #copy speech file from outputs folder to Unity's resources folder\n",
    "    path1 = main_path + 'outputs/speech.wav'\n",
    "    path2 = directory + 'speech.wav'\n",
    "    shutil.copy(path1, path2)\n",
    "\n",
    "    t_emotion = None\n",
    "    t_prob = None\n",
    "    v_emotion = None\n",
    "    v_prob = None\n",
    "    sentence = None\n",
    "    response = None\n",
    "    print(\"wait for 10 seconds\")\n",
    "    i=1\n",
    "    while(i<=10):\n",
    "        print(i)\n",
    "        i+=1\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4352e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebad920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f1a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed94f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaa48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd7253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b574b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
