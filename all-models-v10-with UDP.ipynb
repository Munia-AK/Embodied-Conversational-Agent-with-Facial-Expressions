{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-office",
   "metadata": {},
   "source": [
    "#### combining speech recognition, response generation, emotion recognition models\n",
    "#### talk --> speech to text --> create response --> detect emotion --> send data to unity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-dealing",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-58b91fb9348a>\", line 1, in <module>\n",
      "    from keras.preprocessing.image import img_to_array\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 45, in <module>\n",
      "    from tensorflow.python import data\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\", line 96, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 25, in <module>\n",
      "    from tensorflow.python.data.experimental.ops import compression_ops\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\", line 20, in <module>\n",
      "    from tensorflow.python.data.util import structure\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 26, in <module>\n",
      "    from tensorflow.python.data.util import nest\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 41, in <module>\n",
      "    from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\", line 29, in <module>\n",
      "    from tensorflow.python.framework import constant_op\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 29, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 844, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 939, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1038, in get_data\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\inspect.py\", line 1477, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\Users\\Munia\\.conda\\envs\\VHP\\lib\\tokenize.py\", line 392, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-58b91fb9348a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDispatchServer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoShardPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\VHP\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "import time, logging\n",
    "from datetime import datetime\n",
    "import collections, queue, os, os.path\n",
    "import deepspeech\n",
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "from halo import Halo\n",
    "from scipy import signal\n",
    "\n",
    "import itertools\n",
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "import random\n",
    "import sched\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/Munia/Documents/Anaconda/Master Thesis Project/vh-models/text to speech/TensorFlowTTS-master\")\n",
    "import soundfile as sf\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from tensorflow_tts.inference.auto_model import TFAutoModel\n",
    "from tensorflow_tts.inference.auto_processor import AutoProcessor\n",
    "\n",
    "import server # the server.py script for udp connection\n",
    "import deepspeech_mic_vad_streaming # the mic_vad_streaming script for deepspeech\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e272d15",
   "metadata": {},
   "source": [
    "#### load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'C:/Users/Munia/Documents/Anaconda/Master Thesis Project/vh-models/'\n",
    "\n",
    "\n",
    "# load text based emotion recognition model\n",
    "t_predictor_path = main_path + \"emotion-recognition/emotion recognition from text/Bert/models/bert_model/\"\n",
    "t_predictor = ktrain.load_predictor(t_predictor_path)\n",
    "\n",
    "# load video based emotion recognition models\n",
    "v_detection_model_path = main_path + 'emotion-recognition/real time emotion detection/haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "v_emotion_model_path = main_path + 'emotion-recognition/real time emotion detection/models/XCEPTION-102-0.66.hdf5'\n",
    "face_detection = cv2.CascadeClassifier(v_detection_model_path)\n",
    "v_emotion_classifier = load_model(v_emotion_model_path, compile=False)\n",
    "chatbot_model = 'open-domain-chatbot/models/blender/blender_90M/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-fabric",
   "metadata": {},
   "source": [
    "### speech to text method 1 deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for recognizing speech with deepspeech\n",
    "def speak_1():\n",
    "    DEFAULT_SAMPLE_RATE = 16000\n",
    "    parser = argparse.ArgumentParser(description=\"Stream from microphone to DeepSpeech using VAD\")\n",
    "    parser.add_argument('-v', '--vad_aggressiveness', type=int, default=3,\n",
    "                        help=\"Set aggressiveness of VAD: an integer between 0 and 3, 0 being the least aggressive about filtering out non-speech, 3 the most aggressive. Default: 3\")\n",
    "    parser.add_argument('--nospinner', action='store_true',\n",
    "                        help=\"Disable spinner\")\n",
    "    parser.add_argument('-w', '--savewav',\n",
    "                        help=\"Save .wav files of utterences to given directory\")\n",
    "    parser.add_argument('-f', '--file',\n",
    "                        help=\"Read from .wav file instead of microphone\")\n",
    "    #parser.add_argument('-m', '--model', required=True,\n",
    "    #                    help=\"Path to the model (protocol buffer binary file, or entire directory containing all standard-named files for model)\")\n",
    "    #parser.add_argument('-s', '--scorer',\n",
    "    #                    help=\"Path to the external scorer file.\")\n",
    "    parser.add_argument('-d', '--device', type=int, default=None,\n",
    "                        help=\"Device input index (Int) as listed by pyaudio.PyAudio.get_device_info_by_index(). If not provided, falls back to PyAudio.get_default_device().\")\n",
    "    parser.add_argument('-r', '--rate', type=int, default=DEFAULT_SAMPLE_RATE,\n",
    "                        help=f\"Input device sample rate. Default: {DEFAULT_SAMPLE_RATE}. Your device may require 44100.\")\n",
    "    ARGS = parser.parse_args()\n",
    "    if ARGS.savewav: os.makedirs(ARGS.savewav, exist_ok=True)\n",
    "\n",
    "    deepspeech_mic_vad_streaming.main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a76f9",
   "metadata": {},
   "source": [
    "### speech to text method 2 Google's SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading animation while receiving speech\n",
    "# animation will be for x seconds\n",
    "def loading():\n",
    "    done = False\n",
    "    #here is the animation\n",
    "    def animate():\n",
    "        for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
    "            if done:\n",
    "                break\n",
    "            sys.stdout.write('\\rloading ' + c)\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.1)\n",
    "        sys.stdout.write('\\rDone!     ')\n",
    "\n",
    "    t = threading.Thread(target=animate)\n",
    "    t.start()\n",
    "\n",
    "    #long process here\n",
    "    time.sleep(5)\n",
    "    done = True\n",
    "    \n",
    "\n",
    "# function for recognizing speech\n",
    "# speech recognition stops after x seconds\n",
    "def speak_2():\n",
    "    # obtain audio from the microphone\n",
    "    global sentence\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        #loading()\n",
    "        # stop speaking after 5 seconds\n",
    "        audio = recognizer.listen(source, timeout=5)\n",
    "        print(\"processing..\")\n",
    "    try:\n",
    "        sentence = recognizer.recognize_google(audio)\n",
    "        text_file = open(\"outputs/text-output.txt\", \"w\")\n",
    "        #write string to file\n",
    "        text_file.write(sentence)\n",
    "        #close file\n",
    "        text_file.close()\n",
    "        print(\"***DONE***\\n\\n\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"\\nGoogle Speech Recognition could not understand audio\")\n",
    "        speak()\n",
    "    except sr.RequestError as e:\n",
    "        print(\"\\nCould not request results from Google Speech Recognition service; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219423d",
   "metadata": {},
   "source": [
    "### text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(responsetxt):\n",
    "    # initialize fastspeech2 model.\n",
    "    fastspeech2 = TFAutoModel.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "\n",
    "    # initialize mb_melgan model\n",
    "    mb_melgan = TFAutoModel.from_pretrained(\"tensorspeech/tts-mb_melgan-ljspeech-en\")\n",
    "\n",
    "    # inference\n",
    "    processor = AutoProcessor.from_pretrained(\"tensorspeech/tts-fastspeech2-ljspeech-en\")\n",
    "\n",
    "    input_ids = processor.text_to_sequence(responsetxt)\n",
    "    # fastspeech inference\n",
    "    \n",
    "    mel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(\n",
    "    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\n",
    "    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),\n",
    "    speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
    "    )\n",
    "\n",
    "    # melgan inference\n",
    "    audio_before = mb_melgan.inference(mel_before)[0, :, 0]\n",
    "    audio_after = mb_melgan.inference(mel_after)[0, :, 0]\n",
    "\n",
    "    # save to file\n",
    "    folder = \"F:/Unity/MASTER THESIS/VHProject-master/Assets/Resources\"\n",
    "    #sf.write(folder+'/audio_before.wav', audio_before, 22050, \"PCM_16\")\n",
    "    sf.write(folder+'/demo.wav', audio_after, 22050, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4919c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d1721d",
   "metadata": {},
   "source": [
    "### emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd9f9b",
   "metadata": {},
   "source": [
    "#### from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_based_emotion(text):\n",
    "    global t_emotion\n",
    "    global t_prob\n",
    "    EMOTIONS = ['happy', 'sad', 'scared', 'angry', 'neutral']\n",
    "    \n",
    "    start_time = time.time() \n",
    "    preds = t_predictor.predict(text, return_proba=True)\n",
    "    emotion_probability = np.max(preds)\n",
    "    emotion_probability = str(round(emotion_probability, 2))\n",
    "    #convert probability from string to float\n",
    "    probability = float(emotion_probability)\n",
    "    #print(\"blah blah blah: \", probability)\n",
    "    label = EMOTIONS[preds.argmax()]\n",
    "\n",
    "    #print('predicted: {} {} ({:.2f})'.format(label, emotion_probability, (time.time() - start_time)))\n",
    "    t_emotion = label\n",
    "    t_prob = probability\n",
    "    \n",
    "    #return label, emotion_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video capture is set to play for 5 seconds\n",
    "def video_based_emotion():\n",
    "    \n",
    "    #main_path = 'C:/Users/Munia/Documents/Anaconda/vh-models/emotion-recognition/real time emotion detection/'\n",
    "    global v_emotion\n",
    "    global v_prob\n",
    "    \n",
    "    EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\", \"neutral\"]\n",
    "\n",
    "    cv2.namedWindow('your_face')\n",
    "    t0 = time.time() # start time in seconds\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        frame = camera.read()[1]\n",
    "        #reading the frame\n",
    "        frame = imutils.resize(frame,width=500)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "        frameClone = frame.copy()\n",
    "        if len(faces) > 0:\n",
    "            faces = sorted(faces, reverse=True,\n",
    "            key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "            (fX, fY, fW, fH) = faces\n",
    "\n",
    "            roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "            roi = cv2.resize(roi, (64, 64))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "\n",
    "            preds = v_emotion_classifier.predict(roi)[0]\n",
    "            #print(preds)\n",
    "            emotion_probability = np.max(preds)\n",
    "            #print(np.max(preds))\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "            #print(label)\n",
    "        else: continue\n",
    "\n",
    "        for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                    # construct the label text\n",
    "                    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                    w = int(prob * 300)\n",
    "                    cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
    "                    (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                    cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                    (255, 255, 255), 2)\n",
    "                    cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "                    cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                                  (0, 0, 255), 2)\n",
    "        t1 = time.time() # current time\n",
    "        num_seconds = t1 - t0 # diff\n",
    "\n",
    "        cv2.imshow('your_face', frameClone)\n",
    "        cv2.imshow(\"Probabilities\", canvas)\n",
    "        \n",
    "        # stop video capture after x seconds\n",
    "        if num_seconds > 5:  # e.g. break after 30 seconds\n",
    "            break\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    v_emotion = label\n",
    "    v_prob = emotion_probability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last function for emotion detection that comapres between emotion from video and emotion from text\n",
    "def predict_emotion():\n",
    "    emotionsList = [\"happy\", \"sad\", \"neutral\", \"angry\", \"scared\", \"surprised\", \"disgust\"]\n",
    "    final_emotion = \"\"\n",
    "    final_prob = 0\n",
    "    \n",
    "    if (t_emotion==v_emotion):\n",
    "        final_emotion = t_emotion\n",
    "        final_prob = t_prob\n",
    "        return final_emotion, final_prob\n",
    "    else:\n",
    "        MAX = max(t_prob, v_prob) \n",
    "        if(MAX == t_prob):\n",
    "            final_emotion = t_emotion\n",
    "            final_prob = t_prob\n",
    "            return final_emotion, final_prob\n",
    "        elif(MAX == v_prob):\n",
    "            final_emotion = v_emotion\n",
    "            final_prob = v_prob\n",
    "            return final_emotion, final_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22848dc0",
   "metadata": {},
   "source": [
    "### generate response\n",
    "### response generation with ParlAI Blender 90M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "LocalHumanAgent Class (for keyboard input) to let human reply replacing an ML agent\n",
    "instead of importing  LocalHumanAgent.py Script\n",
    "\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6528e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agent does gets the local keyboard input in the act() function.\n",
    "Example: parlai eval_model -m local_human -t babi:Task1k:1 -dt valid\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.opt import Opt\n",
    "from parlai.core.agents import Agent\n",
    "from parlai.core.message import Message\n",
    "from parlai.utils.misc import display_messages, load_cands\n",
    "from parlai.utils.strings import colorize\n",
    "\n",
    "\n",
    "class LocalHumanAgent(Agent):\n",
    "    @classmethod\n",
    "    def add_cmdline_args(\n",
    "        cls, parser: ParlaiParser, partial_opt: Optional[Opt] = None\n",
    "    ) -> ParlaiParser:\n",
    "        \"\"\"\n",
    "        Add command-line arguments specifically for this agent.\n",
    "        \"\"\"\n",
    "        agent = parser.add_argument_group('Local Human Arguments')\n",
    "        agent.add_argument(\n",
    "            '-fixedCands',\n",
    "            '--local-human-candidates-file',\n",
    "            default=None,\n",
    "            type=str,\n",
    "            help='File of label_candidates to send to other agent',\n",
    "        )\n",
    "        agent.add_argument(\n",
    "            '--single_turn',\n",
    "            type='bool',\n",
    "            default=False,\n",
    "            help='If on, assumes single turn episodes.',\n",
    "        )\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt, shared=None):\n",
    "        super().__init__(opt)\n",
    "        self.id = 'localHuman'\n",
    "        self.episodeDone = False\n",
    "        self.finished = False\n",
    "        self.fixedCands_txt = load_cands(self.opt.get('local_human_candidates_file'))\n",
    "        print(\n",
    "            colorize(\n",
    "                \"Enter [DONE] if you want to end the episode, [EXIT] to quit.\",\n",
    "                'highlight',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def epoch_done(self):\n",
    "        return self.finished\n",
    "\n",
    "    def observe(self, msg):\n",
    "        print(\n",
    "            display_messages(\n",
    "                [msg],\n",
    "                add_fields=self.opt.get('display_add_fields', ''),\n",
    "                prettify=self.opt.get('display_prettify', False),\n",
    "                verbose=self.opt.get('verbose', False),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def act(self):\n",
    "        reply = Message()\n",
    "        reply['id'] = self.getID()\n",
    "        try:\n",
    "            ## input message from user\n",
    "            #reply_text = input(colorize(\"Enter Your Message:\", 'text') + ' ')\n",
    "            reply_text = sentence\n",
    "        except EOFError:\n",
    "            self.finished = True\n",
    "            return {'episode_done': True}\n",
    "\n",
    "        reply_text = reply_text.replace('\\\\n', '\\n')\n",
    "        reply['episode_done'] = False\n",
    "        if self.opt.get('single_turn', False):\n",
    "            reply.force_set('episode_done', True)\n",
    "        reply['label_candidates'] = self.fixedCands_txt\n",
    "        if '[DONE]' in reply_text:\n",
    "            # let interactive know we're resetting\n",
    "            raise StopIteration\n",
    "        reply['text'] = reply_text\n",
    "        print(\"reply['text'] is = \", reply['text'])\n",
    "        print(\"reply is = \", reply)\n",
    "        if '[EXIT]' in reply_text:\n",
    "            self.finished = True\n",
    "            raise StopIteration\n",
    "        return reply\n",
    "\n",
    "    def episode_done(self):\n",
    "        return self.episodeDone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba71fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic script which allows local human keyboard input to talk to a trained model.\n",
    "\n",
    "Model = Blender 90M\n",
    "\"\"\"\n",
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.worlds import create_task\n",
    "from parlai.core.script import ParlaiScript, register_script\n",
    "from parlai.utils.world_logging import WorldLogger\n",
    "import parlai.utils.logging as logging\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def setup_args(parser=None):\n",
    "    if parser is None:\n",
    "        parser = ParlaiParser(\n",
    "            True, True, 'Interactive chat with a model on the command line'\n",
    "        )\n",
    "    parser.add_argument('-d', '--display-examples', type='bool', default=False)\n",
    "    parser.add_argument(\n",
    "        '--display-prettify',\n",
    "        type='bool',\n",
    "        default=False,\n",
    "        help='Set to use a prettytable when displaying '\n",
    "        'examples with text candidates',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--display-add-fields',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Display these fields when verbose is off (e.g., \"--display-add-fields label_candidates,beam_texts\")',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-it',\n",
    "        '--interactive-task',\n",
    "        type='bool',\n",
    "        default=True,\n",
    "        help='Create interactive version of task',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--outfile',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Saves a jsonl file containing all of the task examples and '\n",
    "        'model replies. Set to the empty string to not save at all',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save-format',\n",
    "        type=str,\n",
    "        default='conversations',\n",
    "        choices=['conversations', 'parlai'],\n",
    "        help='Format to save logs in. conversations is a jsonl format, parlai is a text format.',\n",
    "    )\n",
    "    parser.set_defaults(interactive_mode=True, task='interactive')\n",
    "    LocalHumanAgent.add_cmdline_args(parser, partial_opt=None)\n",
    "    WorldLogger.add_cmdline_args(parser, partial_opt=None)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def interactive(opt):\n",
    "    global response\n",
    "    if isinstance(opt, ParlaiParser):\n",
    "        logging.error('interactive should be passed opt not Parser')\n",
    "        opt = opt.parse_args()\n",
    "\n",
    "    # Create model and assign it to the specified task\n",
    "    agent = create_agent(opt, requireModelExists=True)\n",
    "    #agent.opt.log()\n",
    "    human_agent = LocalHumanAgent(opt)\n",
    "    # set up world logger\n",
    "    world_logger = WorldLogger(opt) if opt.get('outfile') else None\n",
    "    world = create_task(opt, [human_agent, agent])\n",
    "\n",
    "    # Show some example dialogs:\n",
    "    world.parley()\n",
    "    #world.get_acts()\n",
    "    response = world.get_acts()[1]['text']\n",
    "    #print(world.get_acts()[1]['text'])\n",
    "    #a = world.get_acts()\n",
    "\n",
    "@register_script('interactive', aliases=['i'])\n",
    "class Interactive(ParlaiScript):\n",
    "    @classmethod\n",
    "    def setup_args(cls):\n",
    "        return setup_args()\n",
    "\n",
    "    def run(self):\n",
    "        return interactive(self.opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responseGenerate(msg):\n",
    "    random.seed(42)\n",
    "    Interactive.main(model_file=chatbot_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a2075",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a5a6f",
   "metadata": {},
   "source": [
    "#### run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  print_dialog():\n",
    "    print(\"\\n\\n____________ Dialog ____________\")\n",
    "    print(\"Me: \", sentence)\n",
    "    print(\"emotion from video is: \", v_emotion)\n",
    "    print(\"probability  is: \", v_prob)\n",
    "\n",
    "    print(\"Bot: \", response)\n",
    "    print(\"emotion from text is: \", t_emotion)\n",
    "    print(\"probability is: \", t_prob)\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Outputs: \")\n",
    "    print(\"final emotion output is: \", final_emo, \"with probability: \", final_prob)\n",
    "    print(\"bot response output is: \", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e86dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "\n",
    "    while(True):\n",
    "        t1 = Thread(target = speak_1)\n",
    "        t2 = Thread(target = video_based_emotion)\n",
    "        \n",
    "        t1.start()\n",
    "        t2.start()\n",
    "        t1.join()\n",
    "        t2.join()\n",
    "        \n",
    "        output_file = open(main_path + \"outputs/text-output.txt\", \"r\")\n",
    "        sentence = output_file.read()\n",
    "        \n",
    "        if(sentence!=None):\n",
    "            \n",
    "            output_file = open(main_path + \"outputs/text-output.txt\", \"r\")\n",
    "            sentence = output_file.read()\n",
    "            output_file.close()\n",
    "    \n",
    "            response = responseGenerate(sentence)\n",
    "            text_based_emotion(response)\n",
    "            \n",
    "            break;\n",
    "            \n",
    "    final_emo, final_prob = predict_emotion()\n",
    "    synthesize(response)\n",
    "    \n",
    "    if(final_emo!=None):\n",
    "        print_dialog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_emo = None\n",
    "t_emotion = None\n",
    "t_prob = None\n",
    "v_emotion = None\n",
    "v_prob = None\n",
    "sentence = None\n",
    "response = None\n",
    "\n",
    "while True:\n",
    "    \n",
    "    data_1 = sock.ReadReceivedData() # get trigger notification from unity\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if data_1 != None: # if NEW data has been received since last ReadReceivedData function call\n",
    "        print(data_1) # print new received data\n",
    "        #run models\n",
    "        pipeline()\n",
    "        #send emotion to unity\n",
    "        sock.SendData(data_2) # Send this string to other application\n",
    "        \n",
    "        final_emo = None\n",
    "        t_emotion = None\n",
    "        t_prob = None\n",
    "        v_emotion = None\n",
    "        v_prob = None\n",
    "        sentence = None\n",
    "        response = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d8cf4",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03940fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a16a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_data(final_emotion):\n",
    "    print(\"script has started\")\n",
    "    # Create UDP socket to use for sending\n",
    "    sock = U.UdpComms(udpIP=\"127.0.0.1\", portTX=8000, portRX=8001, enableRX=True, suppressWarnings=True)\n",
    "    sock.SendData('Sent from Python: ' + str(i)) # Send this string to other application\n",
    "    print(\"!! Data has been sent !!\")\n",
    "    time.sleep(1)\n",
    "\n",
    "def recieve_data():\n",
    "    print(\"script has started\")\n",
    "    # Create UDP socket to use for receiving\n",
    "    sock = U.UdpComms(udpIP=\"127.0.0.1\", portTX=8000, portRX=8001, enableRX=True, suppressWarnings=True)\n",
    "    while True:\n",
    "        data_1 = sock.ReadReceivedData() # read data\n",
    "        print(\"!! Trigger has been recieved !!\")\n",
    "        time.sleep(1)\n",
    "        if data_1 != None: # if NEW data has been received since last ReadReceivedData function call\n",
    "            break\n",
    "    return true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f88ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_emo = None\n",
    "t_emotion = None\n",
    "t_prob = None\n",
    "v_emotion = None\n",
    "v_prob = None\n",
    "sentence = None\n",
    "response = None\n",
    "\n",
    "while(True):\n",
    "    if(recieve_data()==True):\n",
    "        \n",
    "        t1 = Thread(target = speak_1)\n",
    "        t2 = Thread(target = video_based_emotion)\n",
    "\n",
    "        t1.start()\n",
    "        t2.start()\n",
    "        t1.join()\n",
    "        t2.join()\n",
    "\n",
    "        output_file = open(main_path + \"outputs/text-output.txt\", \"r\")\n",
    "        sentence = output_file.read()\n",
    "        output_file.close()\n",
    "\n",
    "        responseGenerate(sentence)\n",
    "        text_based_emotion(response)\n",
    "\n",
    "        final_emo, final_prob = predict_emotion()\n",
    "        synthesize(response)\n",
    "\n",
    "        if(final_emo!=None):\n",
    "            send_data(final_emotion)\n",
    "            print_dialog()\n",
    "            \n",
    "            final_emo = None\n",
    "            t_emotion = None\n",
    "            t_prob = None\n",
    "            v_emotion = None\n",
    "            v_prob = None\n",
    "            sentence = None\n",
    "            response = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96263e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89719c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20235475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpleaudio as sa\n",
    "\n",
    "filename = 'C:/Users/Munia/Documents/Anaconda/Master Thesis Project/vh-models/text to speech/generated_sound/audio_after.wav'\n",
    "wave_obj = sa.WaveObject.from_wave_file(filename)\n",
    "play_obj = wave_obj.play()\n",
    "play_obj.wait_done()  # Wait until sound has finished playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35634b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99204bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
